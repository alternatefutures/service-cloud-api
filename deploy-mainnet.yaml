---
# Akash SDL (Stack Definition Language) for Alternate Futures Backend
# This deploys the GraphQL API + YugabyteDB (3-node cluster) + Self-Hosted IPFS to Akash Network
#
# YugabyteDB replaces both PostgreSQL and Redis with:
# - Distributed SQL database (PostgreSQL-compatible)
# - Built-in high availability (survives 1 node failure)
# - Automatic replication (3x by default)
# - Open source (Apache 2.0)

version: "2.0"

services:
  # YugabyteDB Node 1 - Master + TServer
  # Primary node that other nodes join to
  yb-node-1:
    image: yugabytedb/yugabyte:2.20.1.0-b2
    command:
      - "/home/yugabyte/bin/yugabyted"
      - "start"
      - "--advertise_address=yb-node-1"
      - "--master_flags=replication_factor=3"
      - "--tserver_flags=ysql_enable_auth=true"
      - "--daemon=false"
    env:
      - YSQL_USER=yugabyte
      - YSQL_PASSWORD=your_secure_password_here_change_this
      - YSQL_DB=alternatefutures
    expose:
      # YSQL (PostgreSQL-compatible API) - for internal app connections
      - port: 5433
        as: 5433
        to:
          - service: api
      # YSQL - global TCP access for standalone services (e.g. service-auth)
      # Protected by YSQL_PASSWORD (auth enabled via --tserver_flags=ysql_enable_auth=true)
      - port: 5433
        as: 5433
        proto: tcp
        to:
          - global: true
      # Master RPC - for cluster coordination
      - port: 7100
        as: 7100
        to:
          - service: yb-node-2
          - service: yb-node-3
      # TServer RPC - for data replication
      - port: 9100
        as: 9100
        to:
          - service: yb-node-2
          - service: yb-node-3
      # Admin UI - web interface for monitoring
      - port: 15000
        as: 15000
        to:
          - global: true
        accept:
          - yb.alternatefutures.ai
    params:
      storage:
        data:
          mount: /mnt/disk0
          readOnly: false

  # YugabyteDB Node 2 - Master + TServer
  # Joins node 1 to form HA cluster
  yb-node-2:
    image: yugabytedb/yugabyte:2.20.1.0-b2
    command:
      - "/home/yugabyte/bin/yugabyted"
      - "start"
      - "--advertise_address=yb-node-2"
      - "--join=yb-node-1"
      - "--master_flags=replication_factor=3"
      - "--tserver_flags=ysql_enable_auth=true"
      - "--daemon=false"
    env:
      - YSQL_USER=yugabyte
      - YSQL_PASSWORD=your_secure_password_here_change_this
      - YSQL_DB=alternatefutures
    expose:
      # YSQL - for app connections
      - port: 5433
        as: 5433
        to:
          - service: api
      # Master RPC
      - port: 7100
        as: 7100
        to:
          - service: yb-node-1
          - service: yb-node-3
      # TServer RPC
      - port: 9100
        as: 9100
        to:
          - service: yb-node-1
          - service: yb-node-3
    params:
      storage:
        data:
          mount: /mnt/disk0
          readOnly: false

  # YugabyteDB Node 3 - Master + TServer
  # Third node completes the HA setup (tolerates 1 node failure)
  yb-node-3:
    image: yugabytedb/yugabyte:2.20.1.0-b2
    command:
      - "/home/yugabyte/bin/yugabyted"
      - "start"
      - "--advertise_address=yb-node-3"
      - "--join=yb-node-1"
      - "--master_flags=replication_factor=3"
      - "--tserver_flags=ysql_enable_auth=true"
      - "--daemon=false"
    env:
      - YSQL_USER=yugabyte
      - YSQL_PASSWORD=your_secure_password_here_change_this
      - YSQL_DB=alternatefutures
    expose:
      # YSQL - for app connections
      - port: 5433
        as: 5433
        to:
          - service: api
      # Master RPC
      - port: 7100
        as: 7100
        to:
          - service: yb-node-1
          - service: yb-node-2
      # TServer RPC
      - port: 9100
        as: 9100
        to:
          - service: yb-node-1
          - service: yb-node-2
    params:
      storage:
        data:
          mount: /mnt/disk0
          readOnly: false

  # IPFS Node (Kubo) - Self-hosted decentralized storage
  ipfs:
    image: ipfs/kubo:latest
    env:
      - IPFS_PROFILE=server
      - IPFS_PATH=/data/ipfs
    expose:
      # API port (for API service to connect)
      - port: 5001
        as: 5001
        to:
          - service: api
      # Gateway port (public access to IPFS content)
      - port: 8080
        as: 8080
        to:
          - global: true
        accept:
          - ipfs.alternatefutures.ai
      # Swarm port (P2P connections to IPFS network)
      - port: 4001
        as: 4001
        proto: tcp
        to:
          - global: true
    params:
      storage:
        data:
          mount: /data/ipfs
          readOnly: false

  # Authentication Service
  # Handles user authentication, JWT tokens, magic links
  auth:
    image: ghcr.io/alternatefutures/service-auth:latest
    env:
      # Database - YugabyteDB (PostgreSQL-compatible on port 5433)
      # Can connect to any of the 3 nodes (automatic failover)
      - DATABASE_URL=postgresql://yugabyte:your_secure_password_here_change_this@yb-node-1:5433/alternatefutures

      # App Config
      - NODE_ENV=production
      - PORT=3000

      # JWT Secret (CHANGE THIS! Must match API service)
      - JWT_SECRET=your_jwt_secret_min_32_chars_please_change_this_in_production

      # Email (Resend) - for magic links
      - RESEND_API_KEY=your_resend_api_key

      # Frontend URL - for redirects after auth
      - FRONTEND_URL=https://alternatefutures.ai

      # OpenTelemetry Configuration
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
      - OTEL_SERVICE_NAME=alternatefutures-auth
      - OTEL_TRACES_SAMPLER=always_on
      - OTEL_METRICS_EXPORTER=otlp
      - OTEL_LOGS_EXPORTER=otlp

    expose:
      # Internal port for API service
      - port: 3000
        as: 3000
        to:
          - service: api
      # Public HTTPS endpoint (optional - for direct OAuth/magic link access)
      - port: 3000
        as: 80
        to:
          - global: true
        accept:
          - auth.alternatefutures.ai
    depends_on:
      - yb-node-1
      - yb-node-2
      - yb-node-3

  # GraphQL API Service
  # Connects to YugabyteDB (PostgreSQL-compatible) and IPFS
  api:
    image: ghcr.io/alternatefutures/service-cloud-api:latest
    env:
      # Database - YugabyteDB (PostgreSQL-compatible on port 5433)
      # Can connect to any of the 3 nodes (automatic failover)
      - DATABASE_URL=postgresql://yugabyte:your_secure_password_here_change_this@yb-node-1:5433/alternatefutures

      # App Config
      - NODE_ENV=production
      - PORT=4000

      # JWT Secret (CHANGE THIS!)
      - JWT_SECRET=your_jwt_secret_min_32_chars_please_change_this_in_production

      # Authentication Service
      - AUTH_SERVICE_URL=http://auth:3000

      # Email (Resend)
      - RESEND_API_KEY=your_resend_api_key

      # Storage - Self-Hosted IPFS (Required)
      - IPFS_API_URL=http://ipfs:5001
      - IPFS_GATEWAY_URL=https://ipfs.alternatefutures.ai

      # Storage - Arweave (Optional)
      - ARWEAVE_WALLET=your_arweave_wallet

      # Storage - Filecoin (Direct Integration)
      # Option 1: Run Lotus node (add service above)
      # Option 2: Use public RPC (configure in code)
      - FILECOIN_RPC_URL=https://api.node.glif.io/rpc/v0
      - FILECOIN_WALLET_KEY=your_filecoin_wallet_key

      # OpenTelemetry Configuration
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
      - OTEL_SERVICE_NAME=alternatefutures-api
      - OTEL_TRACES_SAMPLER=always_on
      - OTEL_METRICS_EXPORTER=otlp
      - OTEL_LOGS_EXPORTER=otlp

    expose:
      - port: 4000
        as: 80
        to:
          - global: true
        accept:
          - api.alternatefutures.ai
    depends_on:
      - yb-node-1
      - yb-node-2
      - yb-node-3
      - ipfs
      - auth
      - otel-collector

  # Jaeger - Distributed Tracing UI and Storage
  # All-in-one deployment for beta (includes collector, query, UI)
  jaeger:
    image: jaegertracing/all-in-one:latest
    env:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=false
      - BADGER_DIRECTORY_VALUE=/badger/data
      - BADGER_DIRECTORY_KEY=/badger/key
    expose:
      # Jaeger UI - Web interface for viewing traces
      - port: 16686
        as: 80
        to:
          - global: true
        accept:
          - jaeger.alternatefutures.ai
      # OTLP gRPC receiver (from collector)
      - port: 4317
        as: 4317
        to:
          - service: otel-collector
      # OTLP HTTP receiver (from collector)
      - port: 4318
        as: 4318
        to:
          - service: otel-collector
    params:
      storage:
        data:
          mount: /badger
          readOnly: false

  # OpenTelemetry Collector - Receives and processes telemetry
  # Forwards to Jaeger for storage
  # NOTE: Using custom image with config - build with: docker build -t ghcr.io/alternatefutures/otel-collector:latest docker/otel-collector
  otel-collector:
    image: ghcr.io/alternatefutures/otel-collector:latest
    env:
      - JAEGER_ENDPOINT=jaeger:4317
    expose:
      # OTLP gRPC receiver (from applications)
      - port: 4317
        as: 4317
        to:
          - service: api
          - service: auth
      # OTLP HTTP receiver (from applications)
      - port: 4318
        as: 4318
        to:
          - service: api
          - service: auth
      # Prometheus metrics endpoint
      - port: 8888
        as: 8888
        to:
          - global: true
        accept:
          - otel-metrics.alternatefutures.ai
      # Health check
      - port: 13133
        as: 13133
        to:
          - service: api
          - service: auth
    depends_on:
      - jaeger

profiles:
  compute:
    # YugabyteDB Node 1 (Master + TServer)
    # Beta configuration: 1 CPU per node (TODO: increase to 2 CPUs for production)
    yb-node-1:
      resources:
        cpu:
          units: 1.0
        memory:
          size: 4Gi
        storage:
          # 50Gi for database data + replication
          - name: data
            size: 50Gi

    # YugabyteDB Node 2 (Master + TServer)
    yb-node-2:
      resources:
        cpu:
          units: 1.0
        memory:
          size: 4Gi
        storage:
          - name: data
            size: 50Gi

    # YugabyteDB Node 3 (Master + TServer)
    yb-node-3:
      resources:
        cpu:
          units: 1.0
        memory:
          size: 4Gi
        storage:
          - name: data
            size: 50Gi

    # Auth Service
    auth:
      resources:
        cpu:
          units: 0.5
        memory:
          size: 512Mi
        storage:
          size: 256Mi

    # API Service
    api:
      resources:
        cpu:
          units: 1.0
        memory:
          size: 1Gi
        storage:
          size: 512Mi

    # IPFS Node
    ipfs:
      resources:
        cpu:
          units: 2.0
        memory:
          size: 4Gi
        storage:
          - name: data
            size: 100Gi

    # Jaeger - Tracing storage and UI
    jaeger:
      resources:
        cpu:
          units: 1.0
        memory:
          size: 1.5Gi
        storage:
          - name: data
            size: 10Gi

    # OpenTelemetry Collector
    otel-collector:
      resources:
        cpu:
          units: 0.5
        memory:
          size: 512Mi
        storage:
          size: 256Mi

  placement:
    dcloud:
      pricing:
        # Mainnet-optimized pricing (~270 uakt/block with auth = 116 AKT/month = $70/month)
        # Conservative bids to ensure provider acceptance
        yb-node-1:
          denom: uakt
          amount: 50
        yb-node-2:
          denom: uakt
          amount: 50
        yb-node-3:
          denom: uakt
          amount: 50
        auth:
          denom: uakt
          amount: 20
        api:
          denom: uakt
          amount: 30
        ipfs:
          denom: uakt
          amount: 70
        jaeger:
          denom: uakt
          amount: 35
        otel-collector:
          denom: uakt
          amount: 15

deployment:
  yb-node-1:
    dcloud:
      profile: yb-node-1
      count: 1

  yb-node-2:
    dcloud:
      profile: yb-node-2
      count: 1

  yb-node-3:
    dcloud:
      profile: yb-node-3
      count: 1

  auth:
    dcloud:
      profile: auth
      count: 1

  api:
    dcloud:
      profile: api
      count: 1

  ipfs:
    dcloud:
      profile: ipfs
      count: 1

  jaeger:
    dcloud:
      profile: jaeger
      count: 1

  otel-collector:
    dcloud:
      profile: otel-collector
      count: 1
